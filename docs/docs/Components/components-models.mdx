---
title: Language Model
slug: /components-models
---

import Icon from "@site/src/components/icon";

The Langflow **Language Model** core component generates text using the selected Large Language Model (LLM).

The **Language Model** core component supports many providers and models.
It is appropriate for most text-based language model use cases in Langflow.

If your provider or model isn't supported by the core **Language Model** component, additional single-provider language model components are available in the [**Bundles**](/components-bundle-components) section of the **Components** menu.

## Use the Language Model component in a flow

Use a **Language Model** component anywhere you would use an LLM in a flow.

The component accepts inputs and prompts, including chat messages, files, formatting instructions, and rules, and then generates a text response.
The flow must include [**Chat Input/Output** component](/components-io#chat-io) to allow chat-based interactions with the LLM.
However, you can also use the **Language Model** component for actions requiring an LLM without chat, such as generating text that is processed by another component or triggers another flow before a final response is returned to the user.

This example has the OpenAI model in a chatbot flow. For more information, see the [Basic prompting flow](/basic-prompting).

1. Add the **Language Model** component to your flow.
The default model is OpenAI's GPT-4.1 mini model. Based on [OpenAI's recommendations](https://platform.openai.com/docs/models/gpt-4.1-mini), this model is a good, balanced starter model.
2. In the **OpenAI API Key** field, enter your OpenAI API key.
3. Add a [Prompt](/components-prompts) component to your flow.
4. To connect the [Prompt](/components-prompts) component to the **Language Model** component, on the **Language Model** component, click **Controls**.
5. Enable the **System Message** setting.
On the **Language Model** component, a new **System Message** port opens.
6. Connect the **Prompt** port to the **System Message** port.
7. Add [Chat input](/components-io#chat-input) and [Chat output](/components-io#chat-output) components to your flow.
Your flow looks like this:
![A Language Model component for basic prompting](/img/component-language-model.png)

8. Open the **Playground**, and ask a question.
The bot responds to your question with sources.

    ```
    What is the capital of Utah?

    AI
    gpt-4o-mini
    The capital of Utah is Salt Lake City. It is not only the largest city in the state but also serves as the cultural and economic center of Utah. Salt Lake City was founded in 1847 by Mormon pioneers and is known for its proximity to the Great Salt Lake and its role in the history of the Church of Jesus Christ of Latter-day Saints. For more information, you can refer to sources such as the U.S. Geological Survey or the official state website of Utah.
    ```

9. Try an alternate model provider, and test how the response differs.
In the **Language Model** component, in the **Model Provider** field, select **Anthropic**.
10. In the **Model Name** field, select your Anthropic model.
This model uses Claude 3.5 Haiku, based on [Anthropic's recommendation](https://docs.anthropic.com/en/docs/about-claude/models/choosing-a-model) for a fast and cost-effective model.
11. In the **Anthropic API Key** field, enter your Anthropic API key.
12. Open the **Playground**, and ask the same question as you did before.

    ```
    User
    What is the capital of Utah?

    AI
    claude-3-5-haiku-latest
    The capital of Utah is Salt Lake City. It is also the most populous city in the state. Salt Lake City has been the capital of Utah since 1896, when Utah became a state.
    Sources:
    Utah State Government Official Website (utah.gov)
    U.S. Census Bureau
    Encyclopedia Britannica
    ```

The response from the Anthropic model is less verbose, and lists its sources outside of the informative paragraph.
For more information, see your LLM provider's documentation.

## Language Model parameters

| Name | Type | Description |
|------|------|-------------|
| provider | String | Input parameter. The model provider to use. |
| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |
| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |
| input_value | String | Input parameter. The input text to send to the model. |
| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |
| stream | Boolean | Input parameter. Whether to stream the response. Default: `False`. |
| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |
| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |

## Language Model output types

**Language Model** components, including the core component and bundled components, can produce two types of output:

* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).
Use this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.

* **Language Model**: Change the **Language Model** component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow.
This is a specific data type that is only required by certain components, such as the [**Smart Function** component](/components-processing#smart-function).

    With this configuration, the **Language Model** component is meant to support an action completed by another component, rather than producing a text response for a standard chat-based interaction.
    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.

## Additional language model components

If your provider or model isn't supported by the core **Language Model** component, additional single-provider language model components are available in the [**Bundles**](/components-bundle-components) section of the **Components** menu.