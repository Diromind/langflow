---
title: Language Model
slug: /components-models
---

import Icon from "@site/src/components/icon";

The Langflow **Language Model** core component generates text using the selected Large Language Model (LLM).

The **Language Model** core component supports many providers and models.
It is appropriate for most text-based language model use cases in Langflow.

If your provider or model isn't supported by the core **Language Model** component, additional single-provider language model components are available in the [**Bundles**](/components-bundle-components) section of the **Components** menu.

## Use the Language Model component in a flow

Use a **Language Model** component anywhere you would use an LLM in a flow.

The component accepts inputs and prompts, including chat messages, files, formatting instructions, and rules, and then generates a text response.
The flow must include [**Chat Input/Output** component](/components-io#chat-io) to allow chat-based interactions with the LLM.
However, you can also use the **Language Model** component for actions requiring an LLM without chat, such as generating text that is processed by another component or triggers another flow before a final response is returned to the user.

The following example uses an OpenAI model to create a chatbot flow similar to the [**Basic Prompting** template](/basic-prompting):

1. Add the **Language Model** component to your flow.

2. In the **OpenAI API Key** field, enter your OpenAI API key.

    If you want to use a different provider, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.
    However, these steps use the default OpenAI model specifically as an example to compare response from different providers.

3. In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable the **System Message** parameter, and then click **Close**.

4. Add a [**Prompt Template** component](/components-prompts) to your flow.

5. In the **Template** field, enter some instructions for the LLM, such as `You are an expert in geography who is tutoring high school students`.

6. Connect the **Prompt Template** component's output to the **Language Model** component's **System Message** input.

7. Add [**Chat Input** and **Chat Output** components](/components-io#chat-io) to your flow.

8. Connect the **Chat Input** component to the **Language Model** component's **Input**, and then connect the **Language Model** component's **Message** output to the **Chat Output** component.

    ![A basic prompting flow with Language Model, Prompt Template, Chat Input, and Chat Output components](/img/component-language-model.png)

8. Open the **Playground**, and ask a question to chat with the LLM and test the flow, such as `What is the capital of Utah?`.

    <details>
    <summary>Result</summary>

    The following response is an example of an OpenAI model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    ```
    The capital of Utah is Salt Lake City. It is not only the largest city in the state but also serves as the cultural and economic center of Utah. Salt Lake City was founded in 1847 by Mormon pioneers and is known for its proximity to the Great Salt Lake and its role in the history of the Church of Jesus Christ of Latter-day Saints. For more information, you can refer to sources such as the U.S. Geological Survey or the official state website of Utah.
    ```

    </details>

9. Try a different model or provider to see how the response changes. For example:

    1. In the **Language Model** component, change the model provider to **Anthropic**.
    2. Select an Anthropic model, such as Claude 3.5 Haiku.
    3. Enter an Anthropic API key.

10. Open the **Playground**, ask the same question as you did before, and then compare the content and format of the responses.

    This helps you understand how different models handle the same request so you can choose the best model for your use case.
    You can also learn more about different models in each model provider's documentation.

    <details>
    <summary>Result</summary>

    The following response is an example of an Anthropic model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    Note that this response is shorter and includes sources, whereas the OpenAI response was more encyclopedic and didn't cite sources.

    ```
    The capital of Utah is Salt Lake City. It is also the most populous city in the state. Salt Lake City has been the capital of Utah since 1896, when Utah became a state.
    Sources:
    Utah State Government Official Website (utah.gov)
    U.S. Census Bureau
    Encyclopedia Britannica
    ```

    </details>

## Language Model parameters

| Name | Type | Description |
|------|------|-------------|
| provider | String | Input parameter. The model provider to use. |
| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |
| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |
| input_value | String | Input parameter. The input text to send to the model. |
| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |
| stream | Boolean | Input parameter. Whether to stream the response. Default: `False`. |
| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |
| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |

## Language Model output types

**Language Model** components, including the core component and bundled components, can produce two types of output:

* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).
Use this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.

* **Language Model**: Change the **Language Model** component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow.
This is a specific data type that is only required by certain components, such as the [**Smart Function** component](/components-processing#smart-function).

    With this configuration, the **Language Model** component is meant to support an action completed by another component, rather than producing a text response for a standard chat-based interaction.
    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.

## Additional language model components

If your provider or model isn't supported by the core **Language Model** component, additional single-provider language model components are available in the [**Bundles**](/components-bundle-components) section of the **Components** menu.