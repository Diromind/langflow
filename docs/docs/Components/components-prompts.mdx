---
title: Prompt Template
slug: /components-prompts
---

A prompt is a structured input to a language model that instructs the model how to handle user inputs and variables.

The **Prompt Template** core component creates prompt templates with custom fields and dynamic variables for providing your models structured, repeatable prompts.

Prompts are a combination of natural language and variables created with curly braces.

## Use variables with a Prompt Template component

The [**Basic prompting starter flow** template](/basic-prompting) use a **Prompt Template** component to structure prompts for an LLM.

The flow template's default prompt in the **Prompt Template** component is `Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.`

This prompt creates a role for your LLM's chat interactions, but it doesn't include variables that you may find useful when templating prompts.

To further modify the prompt template, click **Prompt Template** component, and then edit the **Template** field to use variables.

The following example uses a `{context}` variable to give the LLM access to embedded vector data and refine the response:

```text
Given the context
{context}
Answer the question
{user_question}
```

When you add variables to a prompt template, a new field is added to the component for each variable.
These fields can be connected to other components to receive input to populate specific variables in the prompt.
This helps automate prompting and make your templates more dynamic.

The **Prompt Template** component can also output variable instructions to other components later in the flow.

## Prompt Template component parameters

| Name     | Display Name   | Description                                                       |
|----------|----------------|-------------------------------------------------------------------|
| template | Template       | Input parameter. Create a prompt template with dynamic variables. |
| prompt   | Prompt Message | Output parameter. The built prompt message returned by the `build_prompt` method. |

## See also

* [LangChain Prompt Hub](/bundles-langchain#prompt-hub)